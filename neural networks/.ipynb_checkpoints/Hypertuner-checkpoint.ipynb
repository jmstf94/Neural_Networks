{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ae900e-3091-4569-baba-a04ff40b8639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import metrics\n",
    "\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "\n",
    "from TweetDataReport import datasplit, print_tweet_report, check_relevance_balance, datasplit_new\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from hyperopt import hp, fmin, tpe , pyll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786a75d6-fdaa-48a7-83da-5345dd767442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n",
      "TensorFlow Version: 2.10.0\n",
      "GPU is available\n",
      "CUDA Version: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Python Version:\" + sys.version)\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "if tf.test.gpu_device_name():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")\n",
    "print(\"CUDA Version:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4ace83-271d-496a-853e-49a6949c7a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print_tweet_report(WF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f40341-808d-48e9-8d9f-08664434fed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check_relevance_balance(WF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67613ad0-b81b-465a-b5cf-3b5bd7064ee5",
   "metadata": {},
   "source": [
    "## DATA REP PREPARATION (BERT-GPTdone/roberta didn't work due to the same error in every language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d931c4a0-b444-47cc-81b2-1b06e1b9f265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at GroNLP/gpt2-small-italian-embeddings and are newly initialized: ['transformer.gpt2.h.7.mlp.c_fc.weight', 'transformer.gpt2.h.0.attn.c_proj.bias', 'transformer.gpt2.h.6.attn.c_attn.bias', 'transformer.gpt2.h.3.mlp.c_fc.weight', 'transformer.gpt2.h.6.mlp.c_proj.bias', 'transformer.gpt2.h.2.attn.c_attn.weight', 'transformer.gpt2.h.7.ln_2.bias', 'transformer.gpt2.h.0.mlp.c_proj.bias', 'transformer.gpt2.h.6.mlp.c_fc.bias', 'transformer.gpt2.h.10.ln_2.bias', 'transformer.gpt2.h.5.attn.c_attn.bias', 'transformer.gpt2.h.2.mlp.c_fc.weight', 'transformer.gpt2.h.4.mlp.c_fc.bias', 'transformer.gpt2.h.6.ln_1.bias', 'transformer.gpt2.h.2.mlp.c_proj.bias', 'transformer.gpt2.h.4.mlp.c_proj.weight', 'transformer.gpt2.h.0.ln_2.bias', 'transformer.gpt2.h.2.ln_2.weight', 'transformer.gpt2.h.1.attn.c_attn.weight', 'transformer.gpt2.h.0.mlp.c_proj.weight', 'transformer.gpt2.h.5.ln_1.weight', 'transformer.gpt2.h.0.attn.c_attn.weight', 'transformer.gpt2.h.0.attn.c_proj.weight', 'transformer.gpt2.h.7.ln_1.bias', 'transformer.gpt2.h.6.mlp.c_fc.weight', 'transformer.gpt2.h.5.ln_2.weight', 'transformer.gpt2.h.2.ln_1.bias', 'transformer.gpt2.h.7.mlp.c_proj.bias', 'transformer.gpt2.h.11.ln_1.bias', 'transformer.gpt2.h.10.ln_1.weight', 'transformer.gpt2.h.1.ln_1.weight', 'transformer.gpt2.h.8.ln_2.bias', 'transformer.gpt2.h.1.attn.c_proj.bias', 'transformer.gpt2.h.0.ln_1.weight', 'transformer.gpt2.h.9.attn.c_proj.weight', 'transformer.gpt2.h.3.ln_2.weight', 'transformer.gpt2.h.8.attn.c_proj.bias', 'transformer.gpt2.h.2.ln_2.bias', 'transformer.gpt2.h.7.attn.c_proj.bias', 'transformer.gpt2.h.11.mlp.c_fc.bias', 'transformer.gpt2.h.7.ln_1.weight', 'transformer.gpt2.h.3.attn.c_proj.bias', 'transformer.gpt2.h.4.attn.c_proj.weight', 'transformer.gpt2.h.11.attn.c_proj.bias', 'transformer.gpt2.h.3.attn.c_attn.bias', 'transformer.gpt2.h.6.ln_2.bias', 'transformer.gpt2.h.11.ln_2.weight', 'transformer.gpt2.h.2.ln_1.weight', 'transformer.gpt2.h.5.mlp.c_fc.bias', 'transformer.gpt2.h.3.mlp.c_proj.bias', 'transformer.gpt2.h.0.ln_1.bias', 'transformer.gpt2.h.10.ln_1.bias', 'transformer.gpt2.h.0.mlp.c_fc.weight', 'transformer.gpt2.h.8.ln_1.bias', 'transformer.gpt2.h.5.mlp.c_proj.weight', 'transformer.gpt2.h.7.ln_2.weight', 'transformer.gpt2.h.8.mlp.c_proj.bias', 'transformer.gpt2.h.3.ln_1.bias', 'transformer.gpt2.wpe.weight', 'transformer.gpt2.h.1.attn.c_attn.bias', 'transformer.gpt2.h.3.mlp.c_fc.bias', 'transformer.gpt2.h.6.ln_1.weight', 'transformer.gpt2.h.8.ln_1.weight', 'transformer.gpt2.h.7.attn.c_proj.weight', 'transformer.gpt2.h.10.attn.c_attn.bias', 'transformer.gpt2.h.3.attn.c_proj.weight', 'transformer.gpt2.h.8.mlp.c_fc.weight', 'transformer.gpt2.h.9.attn.c_attn.weight', 'transformer.gpt2.h.10.mlp.c_proj.weight', 'transformer.gpt2.h.6.ln_2.weight', 'transformer.gpt2.h.5.attn.c_proj.bias', 'transformer.gpt2.h.1.ln_2.bias', 'transformer.gpt2.h.1.mlp.c_proj.weight', 'transformer.gpt2.h.8.attn.c_attn.weight', 'transformer.gpt2.h.3.ln_2.bias', 'transformer.gpt2.h.8.attn.c_attn.bias', 'transformer.gpt2.h.9.mlp.c_proj.bias', 'transformer.gpt2.h.6.mlp.c_proj.weight', 'transformer.gpt2.h.4.attn.c_attn.bias', 'transformer.gpt2.h.11.mlp.c_fc.weight', 'transformer.gpt2.h.1.attn.c_proj.weight', 'transformer.gpt2.h.0.mlp.c_fc.bias', 'transformer.gpt2.h.9.mlp.c_fc.weight', 'transformer.gpt2.h.10.mlp.c_proj.bias', 'transformer.gpt2.h.5.mlp.c_fc.weight', 'transformer.gpt2.h.7.mlp.c_fc.bias', 'transformer.gpt2.h.8.mlp.c_fc.bias', 'transformer.gpt2.h.8.mlp.c_proj.weight', 'transformer.gpt2.h.1.ln_2.weight', 'transformer.gpt2.h.9.ln_1.bias', 'transformer.gpt2.h.10.mlp.c_fc.bias', 'transformer.gpt2.h.7.attn.c_attn.weight', 'transformer.gpt2.h.10.ln_2.weight', 'transformer.gpt2.h.1.ln_1.bias', 'transformer.gpt2.h.5.ln_1.bias', 'transformer.gpt2.h.3.ln_1.weight', 'transformer.gpt2.h.11.attn.c_proj.weight', 'transformer.gpt2.h.4.ln_1.weight', 'transformer.gpt2.h.2.mlp.c_fc.bias', 'transformer.gpt2.h.10.attn.c_proj.bias', 'transformer.gpt2.h.11.attn.c_attn.weight', 'transformer.gpt2.ln_f.weight', 'transformer.gpt2.h.9.mlp.c_fc.bias', 'transformer.gpt2.h.5.attn.c_proj.weight', 'transformer.gpt2.h.1.mlp.c_proj.bias', 'transformer.gpt2.h.0.attn.c_attn.bias', 'transformer.gpt2.h.5.attn.c_attn.weight', 'transformer.gpt2.h.6.attn.c_proj.bias', 'transformer.gpt2.h.9.ln_1.weight', 'transformer.gpt2.h.9.attn.c_attn.bias', 'transformer.gpt2.h.11.attn.c_attn.bias', 'transformer.gpt2.h.2.attn.c_proj.weight', 'transformer.gpt2.h.2.attn.c_attn.bias', 'transformer.gpt2.h.4.mlp.c_fc.weight', 'transformer.gpt2.h.3.attn.c_attn.weight', 'transformer.gpt2.h.6.attn.c_attn.weight', 'transformer.gpt2.h.4.mlp.c_proj.bias', 'transformer.gpt2.h.10.attn.c_proj.weight', 'transformer.gpt2.h.10.attn.c_attn.weight', 'transformer.gpt2.h.4.ln_2.weight', 'transformer.gpt2.h.2.mlp.c_proj.weight', 'transformer.gpt2.h.9.ln_2.bias', 'transformer.gpt2.h.6.attn.c_proj.weight', 'transformer.gpt2.h.11.mlp.c_proj.bias', 'transformer.gpt2.ln_f.bias', 'transformer.gpt2.h.4.attn.c_proj.bias', 'transformer.gpt2.h.9.attn.c_proj.bias', 'transformer.gpt2.h.11.ln_2.bias', 'transformer.gpt2.h.3.mlp.c_proj.weight', 'transformer.gpt2.h.4.ln_1.bias', 'transformer.gpt2.h.7.attn.c_attn.bias', 'transformer.gpt2.h.8.ln_2.weight', 'transformer.gpt2.h.9.ln_2.weight', 'transformer.gpt2.h.10.mlp.c_fc.weight', 'transformer.gpt2.h.0.ln_2.weight', 'transformer.gpt2.h.5.mlp.c_proj.bias', 'transformer.gpt2.h.2.attn.c_proj.bias', 'transformer.gpt2.h.4.attn.c_attn.weight', 'transformer.gpt2.h.11.ln_1.weight', 'transformer.gpt2.h.4.ln_2.bias', 'transformer.gpt2.wte.weight', 'transformer.gpt2.h.5.ln_2.bias', 'transformer.gpt2.h.8.attn.c_proj.weight', 'transformer.gpt2.h.7.mlp.c_proj.weight', 'transformer.gpt2.h.9.mlp.c_proj.weight', 'transformer.gpt2.h.11.mlp.c_proj.weight', 'transformer.gpt2.h.1.mlp.c_fc.bias', 'transformer.gpt2.h.1.mlp.c_fc.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/italian/04-twisted_remover_with_emoticons/feature_extractions/gpt2/GroNLP_gpt2-small-italian-embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "language =  'italian'\n",
    "#language =  'spanish'\n",
    "#language =  'greek'\n",
    "\n",
    "cleanings = ['00-dirty_dataset','01-basic_remover','02-basic_remover_without_stopwords','03-basic_remover_without_stopwords_with_stemming','04-twisted_remover_with_emoticons']\n",
    "\n",
    "#case = 'bert'\n",
    "#case = 'roberta'\n",
    "case = 'gpt2'\n",
    "\n",
    "# reps_models = [\"dbmdz/bert-base-italian-uncased\",\"dbmdz/bert-base-italian-cased\",\"dbmdz/bert-base-italian-xxl-cased\",\"dbmdz/bert-base-italian-xxl-uncased\"]\n",
    "# reps_models = ['osiria/roberta-base-italian']\n",
    "reps_models = [\"GroNLP/gpt2-small-italian\",\"GroNLP/gpt2-medium-italian-embeddings\",\"GroNLP/gpt2-small-italian-embeddings\"]\n",
    "\n",
    "for combo in itertools.product(cleanings,reps_models):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    with open('data/'+ language +'/'+ combo[0] +'/data.pkl','rb') as file:\n",
    "        WF = pickle.load(file)\n",
    "    \n",
    "    pretrained_model = RepresentationModel(model_type=case,model_name=combo[1],use_cuda=False) \n",
    "    sentence_vectors = pretrained_model.encode_sentences(WF['text'], combine_strategy=\"mean\")\n",
    "    sentence_vectors = np.split(sentence_vectors,sentence_vectors.shape[0])\n",
    "    pandasseries = pd.Series(sentence_vectors)\n",
    "    WF['reps'] = pandasseries.copy()\n",
    "    data = pd.DataFrame()\n",
    "    data['reps'] = WF['reps'].copy()\n",
    "    data['relevance'] = WF['relevance'].copy()\n",
    "    \n",
    "    link = 'data/'+ language +'/'+ combo[0] +'/feature_extractions/'+case+'/'+ str(combo[1]).replace('/', '_')+'.pkl'\n",
    "    \n",
    "    print(link)\n",
    "    \n",
    "    with open(link , 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4635de91-bf56-4022-8c1a-0b2bcd878d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at DeepESP/gpt2-spanish-medium and are newly initialized: ['transformer.gpt2.h.23.ln_2.weight', 'transformer.gpt2.h.19.attn.c_proj.bias', 'transformer.gpt2.h.3.mlp.c_fc.bias', 'transformer.gpt2.wpe.weight', 'transformer.gpt2.h.0.attn.c_attn.weight', 'transformer.gpt2.h.9.mlp.c_proj.bias', 'transformer.gpt2.h.13.attn.c_proj.weight', 'transformer.gpt2.wte.weight', 'transformer.gpt2.h.7.mlp.c_proj.weight', 'transformer.gpt2.h.18.attn.c_proj.weight', 'transformer.gpt2.h.2.ln_2.bias', 'transformer.gpt2.h.2.attn.c_proj.weight', 'transformer.gpt2.h.17.attn.c_proj.weight', 'transformer.gpt2.h.23.ln_1.bias', 'transformer.gpt2.h.6.attn.c_proj.weight', 'transformer.gpt2.h.7.mlp.c_fc.weight', 'transformer.gpt2.h.18.attn.c_attn.bias', 'transformer.gpt2.h.12.attn.c_attn.weight', 'transformer.gpt2.h.10.mlp.c_proj.bias', 'transformer.gpt2.h.12.mlp.c_proj.weight', 'transformer.gpt2.h.18.attn.c_proj.bias', 'transformer.gpt2.h.21.mlp.c_proj.bias', 'transformer.gpt2.h.5.attn.c_proj.bias', 'transformer.gpt2.h.17.ln_1.bias', 'transformer.gpt2.h.0.attn.c_proj.bias', 'transformer.gpt2.h.7.ln_1.bias', 'transformer.gpt2.h.8.ln_1.bias', 'transformer.gpt2.h.7.ln_2.bias', 'transformer.gpt2.h.2.ln_1.bias', 'transformer.gpt2.h.17.ln_2.weight', 'transformer.gpt2.h.17.mlp.c_fc.weight', 'transformer.gpt2.h.0.mlp.c_proj.bias', 'transformer.gpt2.h.22.attn.c_attn.bias', 'transformer.gpt2.h.14.ln_1.bias', 'transformer.gpt2.h.10.mlp.c_proj.weight', 'transformer.gpt2.h.0.ln_1.bias', 'transformer.gpt2.h.20.ln_1.bias', 'transformer.gpt2.h.18.mlp.c_fc.bias', 'transformer.gpt2.h.15.ln_2.bias', 'transformer.gpt2.h.14.attn.c_attn.weight', 'transformer.gpt2.h.4.ln_1.weight', 'transformer.gpt2.h.0.ln_1.weight', 'transformer.gpt2.h.1.attn.c_attn.weight', 'transformer.gpt2.h.20.attn.c_attn.weight', 'transformer.gpt2.h.9.ln_1.bias', 'transformer.gpt2.h.6.mlp.c_proj.weight', 'transformer.gpt2.h.2.attn.c_proj.bias', 'transformer.gpt2.h.9.ln_2.weight', 'transformer.gpt2.h.23.ln_2.bias', 'transformer.gpt2.h.3.attn.c_proj.weight', 'transformer.gpt2.h.23.mlp.c_proj.bias', 'transformer.gpt2.h.2.attn.c_attn.bias', 'transformer.gpt2.h.16.ln_1.weight', 'transformer.gpt2.h.13.ln_1.bias', 'transformer.gpt2.h.8.ln_1.weight', 'transformer.gpt2.h.16.attn.c_attn.weight', 'transformer.gpt2.h.18.mlp.c_proj.weight', 'transformer.gpt2.h.21.ln_2.bias', 'transformer.gpt2.h.23.mlp.c_fc.bias', 'transformer.gpt2.h.5.ln_1.bias', 'transformer.gpt2.h.13.attn.c_proj.bias', 'transformer.gpt2.h.17.mlp.c_proj.weight', 'transformer.gpt2.h.5.mlp.c_fc.bias', 'transformer.gpt2.h.0.ln_2.weight', 'transformer.gpt2.h.19.attn.c_attn.weight', 'transformer.gpt2.h.7.attn.c_attn.bias', 'transformer.gpt2.h.6.mlp.c_fc.weight', 'transformer.gpt2.h.13.ln_1.weight', 'transformer.gpt2.h.6.ln_2.weight', 'transformer.gpt2.h.21.attn.c_proj.weight', 'transformer.gpt2.h.10.ln_2.bias', 'transformer.gpt2.h.21.ln_2.weight', 'transformer.gpt2.h.3.attn.c_attn.bias', 'transformer.gpt2.h.11.mlp.c_fc.weight', 'transformer.gpt2.h.8.mlp.c_fc.weight', 'transformer.gpt2.h.14.ln_2.bias', 'transformer.gpt2.h.2.ln_2.weight', 'transformer.gpt2.h.3.mlp.c_fc.weight', 'transformer.gpt2.h.23.attn.c_attn.weight', 'transformer.gpt2.h.17.attn.c_attn.weight', 'transformer.gpt2.h.8.attn.c_proj.bias', 'transformer.gpt2.h.16.ln_1.bias', 'transformer.gpt2.h.7.mlp.c_fc.bias', 'transformer.gpt2.h.9.mlp.c_fc.bias', 'transformer.gpt2.h.3.attn.c_proj.bias', 'transformer.gpt2.h.14.attn.c_proj.bias', 'transformer.gpt2.h.6.ln_1.bias', 'transformer.gpt2.h.1.ln_1.weight', 'transformer.gpt2.h.11.ln_2.bias', 'transformer.gpt2.h.10.attn.c_attn.bias', 'transformer.gpt2.h.10.ln_2.weight', 'transformer.gpt2.h.13.attn.c_attn.weight', 'transformer.gpt2.h.18.ln_2.bias', 'transformer.gpt2.h.7.attn.c_proj.weight', 'transformer.gpt2.h.5.mlp.c_proj.weight', 'transformer.gpt2.h.11.attn.c_proj.weight', 'transformer.gpt2.h.5.ln_2.weight', 'transformer.gpt2.h.8.attn.c_attn.weight', 'transformer.gpt2.h.12.ln_2.weight', 'transformer.gpt2.h.9.mlp.c_proj.weight', 'transformer.gpt2.h.12.attn.c_attn.bias', 'transformer.gpt2.h.12.attn.c_proj.bias', 'transformer.gpt2.h.14.mlp.c_fc.bias', 'transformer.gpt2.h.4.attn.c_attn.weight', 'transformer.gpt2.h.17.mlp.c_proj.bias', 'transformer.gpt2.h.22.ln_1.weight', 'transformer.gpt2.h.7.attn.c_proj.bias', 'transformer.gpt2.h.22.mlp.c_proj.bias', 'transformer.gpt2.h.11.attn.c_proj.bias', 'transformer.gpt2.h.6.mlp.c_fc.bias', 'transformer.gpt2.h.6.attn.c_attn.bias', 'transformer.gpt2.h.14.ln_1.weight', 'transformer.gpt2.h.9.ln_2.bias', 'transformer.gpt2.h.15.ln_1.weight', 'transformer.gpt2.h.19.mlp.c_fc.weight', 'transformer.gpt2.h.3.ln_2.bias', 'transformer.gpt2.h.4.mlp.c_fc.bias', 'transformer.gpt2.h.17.attn.c_attn.bias', 'transformer.gpt2.h.20.ln_2.weight', 'transformer.gpt2.ln_f.weight', 'transformer.gpt2.h.17.ln_2.bias', 'transformer.gpt2.h.1.ln_2.bias', 'transformer.gpt2.h.13.mlp.c_proj.weight', 'transformer.gpt2.h.7.attn.c_attn.weight', 'transformer.gpt2.h.4.attn.c_proj.bias', 'transformer.gpt2.h.23.attn.c_attn.bias', 'transformer.gpt2.h.22.ln_2.bias', 'transformer.gpt2.h.4.mlp.c_fc.weight', 'transformer.gpt2.h.18.ln_1.bias', 'transformer.gpt2.h.7.ln_2.weight', 'transformer.gpt2.h.9.ln_1.weight', 'transformer.gpt2.h.12.ln_1.weight', 'transformer.gpt2.h.13.mlp.c_proj.bias', 'transformer.gpt2.h.14.mlp.c_proj.weight', 'transformer.gpt2.h.5.attn.c_attn.weight', 'transformer.gpt2.h.5.mlp.c_proj.bias', 'transformer.gpt2.h.12.attn.c_proj.weight', 'transformer.gpt2.h.11.mlp.c_proj.weight', 'transformer.gpt2.h.0.mlp.c_fc.weight', 'transformer.gpt2.h.7.ln_1.weight', 'transformer.gpt2.h.4.ln_2.bias', 'transformer.gpt2.h.7.mlp.c_proj.bias', 'transformer.gpt2.h.13.mlp.c_fc.weight', 'transformer.gpt2.h.19.ln_2.bias', 'transformer.gpt2.h.15.ln_2.weight', 'transformer.gpt2.h.1.attn.c_proj.weight', 'transformer.gpt2.h.18.mlp.c_fc.weight', 'transformer.gpt2.h.11.ln_1.bias', 'transformer.gpt2.h.3.ln_1.weight', 'transformer.gpt2.h.21.attn.c_attn.weight', 'transformer.gpt2.h.8.mlp.c_proj.bias', 'transformer.gpt2.h.1.ln_2.weight', 'transformer.gpt2.h.22.mlp.c_fc.bias', 'transformer.gpt2.h.0.ln_2.bias', 'transformer.gpt2.h.16.attn.c_proj.bias', 'transformer.gpt2.h.10.ln_1.bias', 'transformer.gpt2.h.8.ln_2.bias', 'transformer.gpt2.h.9.attn.c_proj.weight', 'transformer.gpt2.h.10.mlp.c_fc.bias', 'transformer.gpt2.h.4.ln_1.bias', 'transformer.gpt2.h.15.mlp.c_fc.weight', 'transformer.gpt2.h.18.ln_2.weight', 'transformer.gpt2.h.22.attn.c_proj.bias', 'transformer.gpt2.h.23.mlp.c_proj.weight', 'transformer.gpt2.h.16.mlp.c_proj.bias', 'transformer.gpt2.h.13.mlp.c_fc.bias', 'transformer.gpt2.h.2.mlp.c_fc.weight', 'transformer.gpt2.h.12.mlp.c_fc.bias', 'transformer.gpt2.h.8.attn.c_attn.bias', 'transformer.gpt2.h.6.mlp.c_proj.bias', 'transformer.gpt2.h.21.attn.c_attn.bias', 'transformer.gpt2.h.17.ln_1.weight', 'transformer.gpt2.h.13.attn.c_attn.bias', 'transformer.gpt2.h.15.ln_1.bias', 'transformer.gpt2.h.21.mlp.c_fc.bias', 'transformer.gpt2.h.23.attn.c_proj.weight', 'transformer.gpt2.h.11.attn.c_attn.bias', 'transformer.gpt2.h.1.mlp.c_proj.weight', 'transformer.gpt2.h.14.attn.c_proj.weight', 'transformer.gpt2.h.4.ln_2.weight', 'transformer.gpt2.h.9.attn.c_attn.bias', 'transformer.gpt2.h.16.ln_2.weight', 'transformer.gpt2.h.19.ln_2.weight', 'transformer.gpt2.h.1.mlp.c_fc.bias', 'transformer.gpt2.h.20.attn.c_proj.weight', 'transformer.gpt2.h.22.ln_1.bias', 'transformer.gpt2.h.6.attn.c_attn.weight', 'transformer.gpt2.h.4.attn.c_attn.bias', 'transformer.gpt2.h.8.mlp.c_fc.bias', 'transformer.gpt2.h.18.attn.c_attn.weight', 'transformer.gpt2.h.21.mlp.c_proj.weight', 'transformer.gpt2.h.22.mlp.c_proj.weight', 'transformer.gpt2.h.9.mlp.c_fc.weight', 'transformer.gpt2.h.20.attn.c_attn.bias', 'transformer.gpt2.h.13.ln_2.weight', 'transformer.gpt2.h.2.mlp.c_proj.bias', 'transformer.gpt2.h.21.ln_1.weight', 'transformer.gpt2.h.2.mlp.c_fc.bias', 'transformer.gpt2.h.2.ln_1.weight', 'transformer.gpt2.h.12.ln_2.bias', 'transformer.gpt2.h.20.ln_1.weight', 'transformer.gpt2.h.0.attn.c_proj.weight', 'transformer.gpt2.h.12.mlp.c_fc.weight', 'transformer.gpt2.h.11.mlp.c_proj.bias', 'transformer.gpt2.h.10.mlp.c_fc.weight', 'transformer.gpt2.h.8.ln_2.weight', 'transformer.gpt2.h.20.attn.c_proj.bias', 'transformer.gpt2.h.20.ln_2.bias', 'transformer.gpt2.h.14.mlp.c_fc.weight', 'transformer.gpt2.h.15.attn.c_attn.weight', 'transformer.gpt2.h.0.attn.c_attn.bias', 'transformer.gpt2.h.1.mlp.c_proj.bias', 'transformer.gpt2.h.3.mlp.c_proj.bias', 'transformer.gpt2.h.16.mlp.c_fc.weight', 'transformer.gpt2.h.16.mlp.c_proj.weight', 'transformer.gpt2.h.11.ln_2.weight', 'transformer.gpt2.h.22.mlp.c_fc.weight', 'transformer.gpt2.h.5.ln_2.bias', 'transformer.gpt2.h.6.ln_1.weight', 'transformer.gpt2.h.19.ln_1.bias', 'transformer.gpt2.h.0.mlp.c_fc.bias', 'transformer.gpt2.h.4.mlp.c_proj.weight', 'transformer.gpt2.h.4.attn.c_proj.weight', 'transformer.gpt2.h.3.ln_1.bias', 'transformer.gpt2.h.11.mlp.c_fc.bias', 'transformer.gpt2.h.15.attn.c_proj.bias', 'transformer.gpt2.h.15.mlp.c_proj.weight', 'transformer.gpt2.h.16.attn.c_attn.bias', 'transformer.gpt2.h.16.mlp.c_fc.bias', 'transformer.gpt2.h.17.attn.c_proj.bias', 'transformer.gpt2.h.14.attn.c_attn.bias', 'transformer.gpt2.h.9.attn.c_attn.weight', 'transformer.gpt2.h.19.mlp.c_proj.weight', 'transformer.gpt2.h.6.attn.c_proj.bias', 'transformer.gpt2.h.3.ln_2.weight', 'transformer.gpt2.h.16.ln_2.bias', 'transformer.gpt2.h.11.attn.c_attn.weight', 'transformer.gpt2.h.2.mlp.c_proj.weight', 'transformer.gpt2.h.6.ln_2.bias', 'transformer.gpt2.h.18.mlp.c_proj.bias', 'transformer.gpt2.h.21.ln_1.bias', 'transformer.gpt2.h.22.attn.c_attn.weight', 'transformer.gpt2.h.23.attn.c_proj.bias', 'transformer.gpt2.h.10.attn.c_proj.bias', 'transformer.gpt2.h.5.mlp.c_fc.weight', 'transformer.gpt2.h.15.attn.c_attn.bias', 'transformer.gpt2.h.15.attn.c_proj.weight', 'transformer.gpt2.h.8.attn.c_proj.weight', 'transformer.gpt2.h.15.mlp.c_fc.bias', 'transformer.gpt2.h.22.ln_2.weight', 'transformer.gpt2.h.1.attn.c_proj.bias', 'transformer.gpt2.h.19.attn.c_proj.weight', 'transformer.gpt2.h.1.ln_1.bias', 'transformer.gpt2.h.11.ln_1.weight', 'transformer.gpt2.h.3.mlp.c_proj.weight', 'transformer.gpt2.h.5.attn.c_attn.bias', 'transformer.gpt2.h.23.mlp.c_fc.weight', 'transformer.gpt2.h.23.ln_1.weight', 'transformer.gpt2.h.5.ln_1.weight', 'transformer.gpt2.h.19.ln_1.weight', 'transformer.gpt2.h.0.mlp.c_proj.weight', 'transformer.gpt2.h.20.mlp.c_proj.weight', 'transformer.gpt2.h.2.attn.c_attn.weight', 'transformer.gpt2.h.10.attn.c_attn.weight', 'transformer.gpt2.h.10.ln_1.weight', 'transformer.gpt2.h.19.mlp.c_proj.bias', 'transformer.gpt2.h.20.mlp.c_fc.bias', 'transformer.gpt2.h.3.attn.c_attn.weight', 'transformer.gpt2.h.4.mlp.c_proj.bias', 'transformer.gpt2.h.13.ln_2.bias', 'transformer.gpt2.h.14.ln_2.weight', 'transformer.gpt2.h.17.mlp.c_fc.bias', 'transformer.gpt2.h.18.ln_1.weight', 'transformer.gpt2.h.9.attn.c_proj.bias', 'transformer.gpt2.h.12.ln_1.bias', 'transformer.gpt2.h.1.mlp.c_fc.weight', 'transformer.gpt2.h.19.attn.c_attn.bias', 'transformer.gpt2.h.20.mlp.c_proj.bias', 'transformer.gpt2.h.12.mlp.c_proj.bias', 'transformer.gpt2.h.15.mlp.c_proj.bias', 'transformer.gpt2.h.19.mlp.c_fc.bias', 'transformer.gpt2.h.10.attn.c_proj.weight', 'transformer.gpt2.h.14.mlp.c_proj.bias', 'transformer.gpt2.h.21.attn.c_proj.bias', 'transformer.gpt2.h.5.attn.c_proj.weight', 'transformer.gpt2.h.16.attn.c_proj.weight', 'transformer.gpt2.h.20.mlp.c_fc.weight', 'transformer.gpt2.h.22.attn.c_proj.weight', 'transformer.gpt2.h.1.attn.c_attn.bias', 'transformer.gpt2.ln_f.bias', 'transformer.gpt2.h.8.mlp.c_proj.weight', 'transformer.gpt2.h.21.mlp.c_fc.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/spanish/04-twisted_remover_with_emoticons/feature_extractions/gpt2/DeepESP_gpt2-spanish-medium.pkl\n"
     ]
    }
   ],
   "source": [
    "#language =  'italian'\n",
    "language =  'spanish'\n",
    "#language =  'greek'\n",
    "\n",
    "cleanings = ['00-dirty_dataset','01-basic_remover','02-basic_remover_without_stopwords','03-basic_remover_without_stopwords_with_stemming','04-twisted_remover_with_emoticons']\n",
    "\n",
    "#case = 'bert'\n",
    "#case = 'roberta'\n",
    "case = 'gpt2'\n",
    "\n",
    "#reps_models = [\"dccuchile/bert-base-spanish-wwm-uncased\",\"dccuchile/bert-base-spanish-wwm-cased\",\"Geotrend/bert-base-es-cased\",\"dccuchile/tulio-chilean-spanish-bert\",\"dccuchile/patana-chilean-spanish-bert\"]\n",
    "#reps_models = [\"MMG/mlm-spanish-roberta-base\",\"llange/xlm-roberta-large-spanish\"]\n",
    "reps_models = [\"DeepESP/gpt2-spanish\",\"datificate/gpt2-small-spanish\",\"mrm8488/spanish-gpt2\",\"DeepESP/gpt2-spanish-medium\"]\n",
    "\n",
    "for combo in itertools.product(cleanings,reps_models):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    with open('data/'+ language +'/'+ combo[0] +'/data.pkl','rb') as file:\n",
    "        WF = pickle.load(file)\n",
    "    \n",
    "    pretrained_model = RepresentationModel(model_type=case,model_name=combo[1],use_cuda=False) \n",
    "    sentence_vectors = pretrained_model.encode_sentences(WF['text'], combine_strategy=\"mean\")\n",
    "    sentence_vectors = np.split(sentence_vectors,sentence_vectors.shape[0])\n",
    "    pandasseries = pd.Series(sentence_vectors)\n",
    "    WF['reps'] = pandasseries.copy()\n",
    "    data = pd.DataFrame()\n",
    "    data['reps'] = WF['reps'].copy()\n",
    "    data['relevance'] = WF['relevance'].copy()\n",
    "\n",
    "    link = 'data/'+ language +'/'+ combo[0] +'/feature_extractions/'+case+'/'+ str(combo[1]).replace('/', '_')+'.pkl'\n",
    "    \n",
    "    print(link)\n",
    "    \n",
    "    with open(link , 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81fa9fd-91b1-48f0-9a3a-82a93bc1d718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at ClassCat/gpt2-small-greek-v2 and are newly initialized: ['transformer.gpt2.h.3.mlp.c_fc.bias', 'transformer.gpt2.wpe.weight', 'transformer.gpt2.h.0.attn.c_attn.weight', 'transformer.gpt2.wte.weight', 'transformer.gpt2.h.7.mlp.c_proj.weight', 'transformer.gpt2.h.2.ln_2.bias', 'transformer.gpt2.h.2.attn.c_proj.weight', 'transformer.gpt2.h.6.attn.c_proj.weight', 'transformer.gpt2.h.7.mlp.c_fc.weight', 'transformer.gpt2.h.5.attn.c_proj.bias', 'transformer.gpt2.h.0.attn.c_proj.bias', 'transformer.gpt2.h.7.ln_1.bias', 'transformer.gpt2.h.7.ln_2.bias', 'transformer.gpt2.h.2.ln_1.bias', 'transformer.gpt2.h.0.mlp.c_proj.bias', 'transformer.gpt2.h.0.ln_1.bias', 'transformer.gpt2.h.4.ln_1.weight', 'transformer.gpt2.h.0.ln_1.weight', 'transformer.gpt2.h.1.attn.c_attn.weight', 'transformer.gpt2.h.6.mlp.c_proj.weight', 'transformer.gpt2.h.2.attn.c_proj.bias', 'transformer.gpt2.h.3.attn.c_proj.weight', 'transformer.gpt2.h.2.attn.c_attn.bias', 'transformer.gpt2.h.5.ln_1.bias', 'transformer.gpt2.h.5.mlp.c_fc.bias', 'transformer.gpt2.h.0.ln_2.weight', 'transformer.gpt2.h.7.attn.c_attn.bias', 'transformer.gpt2.h.6.mlp.c_fc.weight', 'transformer.gpt2.h.6.ln_2.weight', 'transformer.gpt2.h.3.attn.c_attn.bias', 'transformer.gpt2.h.2.ln_2.weight', 'transformer.gpt2.h.3.mlp.c_fc.weight', 'transformer.gpt2.h.7.mlp.c_fc.bias', 'transformer.gpt2.h.3.attn.c_proj.bias', 'transformer.gpt2.h.6.ln_1.bias', 'transformer.gpt2.h.1.ln_1.weight', 'transformer.gpt2.h.7.attn.c_proj.weight', 'transformer.gpt2.h.5.mlp.c_proj.weight', 'transformer.gpt2.h.5.ln_2.weight', 'transformer.gpt2.h.4.attn.c_attn.weight', 'transformer.gpt2.h.7.attn.c_proj.bias', 'transformer.gpt2.h.6.mlp.c_fc.bias', 'transformer.gpt2.h.6.attn.c_attn.bias', 'transformer.gpt2.h.3.ln_2.bias', 'transformer.gpt2.h.4.mlp.c_fc.bias', 'transformer.gpt2.ln_f.weight', 'transformer.gpt2.h.1.ln_2.bias', 'transformer.gpt2.h.7.attn.c_attn.weight', 'transformer.gpt2.h.4.attn.c_proj.bias', 'transformer.gpt2.h.4.mlp.c_fc.weight', 'transformer.gpt2.h.7.ln_2.weight', 'transformer.gpt2.h.5.attn.c_attn.weight', 'transformer.gpt2.h.5.mlp.c_proj.bias', 'transformer.gpt2.h.0.mlp.c_fc.weight', 'transformer.gpt2.h.7.mlp.c_proj.bias', 'transformer.gpt2.h.7.ln_1.weight', 'transformer.gpt2.h.4.ln_2.bias', 'transformer.gpt2.h.1.attn.c_proj.weight', 'transformer.gpt2.h.3.ln_1.weight', 'transformer.gpt2.h.1.ln_2.weight', 'transformer.gpt2.h.0.ln_2.bias', 'transformer.gpt2.h.4.ln_1.bias', 'transformer.gpt2.h.2.mlp.c_fc.weight', 'transformer.gpt2.h.6.mlp.c_proj.bias', 'transformer.gpt2.h.1.mlp.c_proj.weight', 'transformer.gpt2.h.4.ln_2.weight', 'transformer.gpt2.h.1.mlp.c_fc.bias', 'transformer.gpt2.h.6.attn.c_attn.weight', 'transformer.gpt2.h.4.attn.c_attn.bias', 'transformer.gpt2.h.2.mlp.c_proj.bias', 'transformer.gpt2.h.2.mlp.c_fc.bias', 'transformer.gpt2.h.2.ln_1.weight', 'transformer.gpt2.h.0.attn.c_proj.weight', 'transformer.gpt2.h.0.attn.c_attn.bias', 'transformer.gpt2.h.1.mlp.c_proj.bias', 'transformer.gpt2.h.3.mlp.c_proj.bias', 'transformer.gpt2.h.5.ln_2.bias', 'transformer.gpt2.h.6.ln_1.weight', 'transformer.gpt2.h.0.mlp.c_fc.bias', 'transformer.gpt2.h.4.mlp.c_proj.weight', 'transformer.gpt2.h.4.attn.c_proj.weight', 'transformer.gpt2.h.3.ln_1.bias', 'transformer.gpt2.h.6.attn.c_proj.bias', 'transformer.gpt2.h.3.ln_2.weight', 'transformer.gpt2.h.2.mlp.c_proj.weight', 'transformer.gpt2.h.6.ln_2.bias', 'transformer.gpt2.h.5.mlp.c_fc.weight', 'transformer.gpt2.h.1.attn.c_proj.bias', 'transformer.gpt2.h.1.ln_1.bias', 'transformer.gpt2.h.3.mlp.c_proj.weight', 'transformer.gpt2.h.5.attn.c_attn.bias', 'transformer.gpt2.h.5.ln_1.weight', 'transformer.gpt2.h.0.mlp.c_proj.weight', 'transformer.gpt2.h.2.attn.c_attn.weight', 'transformer.gpt2.h.3.attn.c_attn.weight', 'transformer.gpt2.h.4.mlp.c_proj.bias', 'transformer.gpt2.h.1.mlp.c_fc.weight', 'transformer.gpt2.h.5.attn.c_proj.weight', 'transformer.gpt2.h.1.attn.c_attn.bias', 'transformer.gpt2.ln_f.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/greek/04-twisted_remover_with_emoticons/feature_extractions/gpt2/ClassCat_gpt2-small-greek-v2.pkl\n"
     ]
    }
   ],
   "source": [
    "#language =  'italian'\n",
    "#language =  'spanish'\n",
    "language =  'greek'\n",
    "\n",
    "cleanings = ['00-dirty_dataset','01-basic_remover','02-basic_remover_without_stopwords','03-basic_remover_without_stopwords_with_stemming','04-twisted_remover_with_emoticons']\n",
    "\n",
    "#model_type = 'bert'\n",
    "#case = 'roberta'\n",
    "model_type = 'gpt2'\n",
    "\n",
    "#reps_models = [\"Geotrend/bert-base-el-cased\",\"nlpaueb/bert-base-greek-uncased-v1\",\"dimitriz/st-greek-media-bert-base-uncased\",\"petros/bert-base-cypriot-uncased-v1\"]\n",
    "reps_models = [\"lighteternal/gpt2-finetuned-greek-small\",\"lighteternal/gpt2-finetuned-greek\",\"nikokons/gpt2-greek\",\"ClassCat/gpt2-small-greek-v2\"]\n",
    "\n",
    "for combo in itertools.product(cleanings,reps_models):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    with open('data/'+ language +'/'+ combo[0] +'/data.pkl','rb') as file:\n",
    "        WF = pickle.load(file)\n",
    "    \n",
    "    pretrained_model = RepresentationModel(model_type=model_type,model_name=combo[1],use_cuda=False) \n",
    "    sentence_vectors = pretrained_model.encode_sentences(WF['text'], combine_strategy=\"mean\")\n",
    "    sentence_vectors = np.split(sentence_vectors,sentence_vectors.shape[0])\n",
    "    pandasseries = pd.Series(sentence_vectors)\n",
    "    WF['reps'] = pandasseries.copy()\n",
    "    data = pd.DataFrame()\n",
    "    data['reps'] = WF['reps'].copy()\n",
    "    data['relevance'] = WF['relevance'].copy()\n",
    "    \n",
    "    link = 'data/'+ language +'/'+ combo[0] +'/feature_extractions/'+model_type+'/'+ str(combo[1]).replace('/', '_')+'.pkl'\n",
    "    \n",
    "    print(link)\n",
    "    \n",
    "    with open(link , 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ce0ce-fa25-4bb8-baec-c76a44a0d622",
   "metadata": {},
   "source": [
    "## POST PROCESSING HYPER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de36109-af3a-4157-8773-a8f6ea096244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/italian/00-dirty_dataset/feature_extractions/bert/dbmdz_bert-base-italian-cased.pkl\n",
      "data/italian/00-dirty_dataset/feature_extractions/bert/dbmdz_bert-base-italian-uncased.pkl\n",
      "data/italian/00-dirty_dataset/feature_extractions/bert/dbmdz_bert-base-italian-xxl-cased.pkl\n",
      "data/italian/00-dirty_dataset/feature_extractions/bert/dbmdz_bert-base-italian-xxl-uncased.pkl\n",
      "data/italian/01-basic_remover/feature_extractions/bert/dbmdz_bert-base-italian-cased.pkl\n",
      "data/italian/01-basic_remover/feature_extractions/bert/dbmdz_bert-base-italian-uncased.pkl\n",
      "data/italian/01-basic_remover/feature_extractions/bert/dbmdz_bert-base-italian-xxl-cased.pkl\n",
      "data/italian/01-basic_remover/feature_extractions/bert/dbmdz_bert-base-italian-xxl-uncased.pkl\n",
      "data/italian/02-basic_remover_without_stopwords/feature_extractions/bert/dbmdz_bert-base-italian-cased.pkl\n",
      "data/italian/02-basic_remover_without_stopwords/feature_extractions/bert/dbmdz_bert-base-italian-uncased.pkl\n",
      "data/italian/02-basic_remover_without_stopwords/feature_extractions/bert/dbmdz_bert-base-italian-xxl-cased.pkl\n",
      "data/italian/02-basic_remover_without_stopwords/feature_extractions/bert/dbmdz_bert-base-italian-xxl-uncased.pkl\n",
      "data/italian/03-basic_remover_without_stopwords_with_stemming/feature_extractions/bert/dbmdz_bert-base-italian-cased.pkl\n",
      "data/italian/03-basic_remover_without_stopwords_with_stemming/feature_extractions/bert/dbmdz_bert-base-italian-uncased.pkl\n",
      "data/italian/03-basic_remover_without_stopwords_with_stemming/feature_extractions/bert/dbmdz_bert-base-italian-xxl-cased.pkl\n",
      "data/italian/03-basic_remover_without_stopwords_with_stemming/feature_extractions/bert/dbmdz_bert-base-italian-xxl-uncased.pkl\n",
      "data/italian/04-twisted_remover_with_emoticons/feature_extractions/bert/dbmdz_bert-base-italian-cased.pkl\n",
      "data/italian/04-twisted_remover_with_emoticons/feature_extractions/bert/dbmdz_bert-base-italian-uncased.pkl\n",
      "data/italian/04-twisted_remover_with_emoticons/feature_extractions/bert/dbmdz_bert-base-italian-xxl-cased.pkl\n",
      "data/italian/04-twisted_remover_with_emoticons/feature_extractions/bert/dbmdz_bert-base-italian-xxl-uncased.pkl\n"
     ]
    }
   ],
   "source": [
    "# I just change the combinations of language and model_type\n",
    "\n",
    "language =  'italian'\n",
    "#language =  'spanish'\n",
    "#language =  'greek'\n",
    "\n",
    "cleanings = ['00-dirty_dataset','01-basic_remover','02-basic_remover_without_stopwords','03-basic_remover_without_stopwords_with_stemming','04-twisted_remover_with_emoticons']\n",
    "\n",
    "model_type = 'bert'\n",
    "# ase = 'roberta' didn't manage to get representations yet\n",
    "#case = 'gpt-2' probably the code needs reformation\n",
    "\n",
    "for cleaning in cleanings:\n",
    "    path = 'data/'+ language +'/'+ cleaning + '/feature_extractions/' + model_type\n",
    "    for x in os.listdir(path):\n",
    "        file = path + '/' + x\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f16d98-1d39-4ce1-bdcf-ab1dbb1060b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = 'data/italian/00-dirty_dataset/feature_extractions/bert/dbmdz_bert-base-italian-cased.pkl'\n",
    "\n",
    "with open(file, \"rb\") as file:\n",
    "    WF = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f719d-ad77-4b91-8f9a-b03d599d06b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_names</th>\n",
       "      <th>data_types</th>\n",
       "      <th>shape_len</th>\n",
       "      <th>unique_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reps</td>\n",
       "      <td>&lt;class 'numpy.ndarray'&gt;</td>\n",
       "      <td>(1, 768)</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevance</td>\n",
       "      <td>&lt;class 'numpy.int64'&gt;</td>\n",
       "      <td>()</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_names               data_types shape_len  unique_values\n",
       "0         reps  <class 'numpy.ndarray'>  (1, 768)           1892\n",
       "1    relevance    <class 'numpy.int64'>        ()              2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_tweet_report(WF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccdb1cc-c535-4980-a869-ec7e8d8bf8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>count</th>\n",
       "      <th>balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1258</td>\n",
       "      <td>66.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>634</td>\n",
       "      <td>33.51%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance  count balance\n",
       "0          0   1258  66.49%\n",
       "1          1    634  33.51%"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_relevance_balance(WF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55c0c41-bed1-4aaf-972d-a8b1018a01f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperspace = {\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.5),\n",
    "    'dropout_rate2': hp.uniform('dropout_rate2', 0.1, 0.5),\n",
    "    'dropout_rate3': hp.uniform('dropout_rate3', 0.1, 0.5),\n",
    "    'dropout_rate4': hp.uniform('dropout_rate4', 0.1, 0.5),\n",
    "    'dropout_rate5': hp.uniform('dropout_rate6', 0.1, 0.5),\n",
    "    'relovir': hp.uniform('relovir', 0.1, 0.5),\n",
    "    'PostEmbedding#nods': hp.quniform('PostEmbedding#nods', 1 , 768, q=1),\n",
    "    'batch_size' : hp.quniform('batch_size',20,140,q=5),\n",
    "    'PostEmbedding#nods2': hp.quniform('PostEmbedding#nods2', 1 , 768, q=1),\n",
    "    'PostEmbedding#nods3': hp.quniform('PostEmbedding#nods3', 1 , 768, q=1),\n",
    "    'PostEmbedding#nods4': hp.quniform('PostEmbedding#nods4', 1 , 768, q=1),\n",
    "    'PostEmbedding#nods5': hp.quniform('PostEmbedding#nods5', 1 , 768, q=1),\n",
    "    'PrePredictionActivation':hp.choice('PrePredictionActivation',['softmax','sigmoid'])\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f967b08-62de-4a58-9093-879e0c8c64b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class Relovir_Error(Exception):\n",
    "    \"\"\"\n",
    "    This class checks if the given relovir number is acceptable based on the ratio of the dataset you gave.\n",
    "    The dataset of course must be a pandas dataframe with two columns.The second column must be the 0\n",
    "    (irrelevant) or 1(relevant) values of the corresponding tweets/representations. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,dataset,relovir):\n",
    "        self.relovir = relovir\n",
    "        dataset = dataset.copy()\n",
    "        checker = check_relevance_balance(dataset).copy()\n",
    "        self.irr = checker.loc[checker['relevance']==0,'count'].reset_index(drop=True)[0]\n",
    "        self.rel = checker.loc[checker['relevance']==1,'count'].reset_index(drop=True)[0]\n",
    "        self.ratio = self.rel/self.irr\n",
    "        if self.relovir>self.ratio or self.relovir>1:\n",
    "            raise self\n",
    "    def __str__(self):\n",
    "        if self.relovir>self.ratio:\n",
    "            return f\"The relovir ratio of your dataset is {self.ratio} but you gave me {self.relovir}. The relovir ratio cannot be larger than 1.\"\n",
    "        \n",
    "def datasplit_new(df,testsize,relovir=None):\n",
    "    \"\"\"\n",
    "    Firstly we split the dataset into train and test parts.\n",
    "    \n",
    "    Then we create the training dataset by picking up the irrelevant tweets from the training \n",
    "    split part with only the number of relevant tweet\n",
    "    \n",
    "    The relovir variable represents the relative ratio of irrelevant(we usually have more irrelevant so) \n",
    "    over relevant number of training examples in the set.\n",
    "    \n",
    "    Returns as a (examples,768) np array the representations and the y as (examples,) shaped np array.\n",
    "    \n",
    "    Future: maybe it would be better to split the dataset by relevance and then pick up the \"correct\" \n",
    "    relovir ratio for the test dataset from the ratio of the total dataset. \n",
    "    Now we include some randomness which is not particularly wanted due to the fact that after the split\n",
    "    the relovir ratios of the training and test parts won't match exactly. \n",
    "    I will have to make 20 30 iterations per model to make sure we get the average.\n",
    "    In the other case we were going to be satisfied with 5. \n",
    "    \"\"\"\n",
    "    df = df.copy() # make a copied instance of the dataset\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['reps'], df['relevance'], test_size = testsize)\n",
    "\n",
    "    if relovir is not None:\n",
    "        \n",
    "        Relovir_Error(df,relovir) # check if you can accept the relovir variable instance maybe the ratio is not that big\n",
    "        \n",
    "        # now we reconstruct the training dataset in order to use the relovir \n",
    "        training_set = pd.DataFrame()\n",
    "        training_set['reps'] = X_train\n",
    "        training_set['relevance'] = y_train\n",
    "        training_set.reset_index(drop = True)\n",
    "        # we split the training data in irrelevant and relevant cases\n",
    "        # we make sure the zeros and the ones correctly correspond to irr and rel respectively\n",
    "        grouping = training_set.groupby('relevance')\n",
    "        group_dict = {}\n",
    "        for name, group in grouping:\n",
    "            group_dict[str(name)] = group\n",
    "        # we find the absolute numbers \n",
    "        irr = group_dict['0']\n",
    "        rel = group_dict['1']\n",
    "        #print(len(irr)+len(rel))\n",
    "        #pickup all the irrelevants and the correct random part of the relevants\n",
    "        dfirr = irr.sample(frac = 1).reset_index(drop = True)\n",
    "        #print(len(dfirr))\n",
    "        dfrel = rel.sample(n = int(len(irr)*relovir)).reset_index(drop = True)\n",
    "        #print(len(dfrel))\n",
    "        #print(len(dfirr)+len(dfrel)\n",
    "        training_set = None\n",
    "        training_set = pd.concat([dfirr, dfrel]).sample(frac = 1).reset_index(drop = True)\n",
    "    training_set_X = np.vstack(training_set['reps'])\n",
    "    test_set_X = np.vstack(X_test)\n",
    "    y_train = (training_set['relevance']).to_frame().reset_index(drop = True)\n",
    "    y_test = y_test.to_frame().reset_index(drop = True)\n",
    "    return training_set_X, test_set_X, y_train.to_numpy().reshape(-1), y_test.to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66a8be7d-f2dd-4d3d-bf4d-2e62a417b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_X, test_set_X, training_set_y, test_set_y = datasplit(WF,TEST_SIZE,relovir = RELOVIR)\n",
    "training_set_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad61bba9-7874-418e-9a76-70636f0612fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelize(calibrers):  \n",
    "    \n",
    "    hyperparameters = {\n",
    "                   'ITERATIONS':'30',\n",
    "                   'EPOCHS': '50',\n",
    "                   'LOSS':\"'binary_crossentropy'\",\n",
    "                   'OPTIMIZER': 'Adam(learning_rate=0.0001)',\n",
    "                   'METRICS': \"\"\"['acc', metrics.precision, metrics.recall, metrics.f1]\"\"\",\n",
    "                   'BATCH_SIZE': \"\"\"calibrers['batch_size']\"\"\",\n",
    "                   'TEST_SIZE': '0.3',\n",
    "                   'RELOVIR': \"\"\"calibrers['relovir']\"\"\"}\n",
    "    \n",
    "    for key, value in hyperparameters.items():\n",
    "        globals()[key] = eval(value)\n",
    "    \n",
    "    av_loss_train = np.zeros(EPOCHS)\n",
    "    av_loss_val = np.zeros(EPOCHS)\n",
    "    av_acc_train = np.zeros(EPOCHS)\n",
    "    av_acc_val = np.zeros(EPOCHS)\n",
    "    av_prec_train = np.zeros(EPOCHS)\n",
    "    av_prec_val = np.zeros(EPOCHS)\n",
    "    av_rec_train = np.zeros(EPOCHS)\n",
    "    av_rec_val = np.zeros(EPOCHS)\n",
    "    av_f1_train = np.zeros(EPOCHS)\n",
    "    av_f1_val = np.zeros(EPOCHS)\n",
    "    \n",
    "    iter_time = 0\n",
    "    \n",
    "    for iteration in range(ITERATIONS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        training_set_X, test_set_X, training_set_y, test_set_y = datasplit(WF,TEST_SIZE,relovir = RELOVIR)\n",
    "        \n",
    "        print(training_set_X.shape)\n",
    "        \n",
    "        model = None\n",
    "    \n",
    "        #np.random.seed(SEED)\n",
    "    \n",
    "        #initializer choice\n",
    "    \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Input(shape = (768,)))\n",
    "        \n",
    "        model.add(Dense(calibrers['PostEmbedding#nods'],activation='relu'))\n",
    "        model.add(Dropout(calibrers['dropout_rate']))\n",
    "        \n",
    "        model.add(Dense(calibrers['PostEmbedding#nods2'],activation='relu'))\n",
    "        model.add(Dropout(calibrers['dropout_rate2']))\n",
    "        \n",
    "        model.add(Dense(calibrers['PostEmbedding#nods3'],activation='relu'))\n",
    "        model.add(Dropout(calibrers['dropout_rate3']))\n",
    "        \n",
    "        model.add(Dense(calibrers['PostEmbedding#nods4'],activation='relu'))\n",
    "        model.add(Dropout(calibrers['dropout_rate4']))\n",
    "        \n",
    "        model.add(Dense(calibrers['PostEmbedding#nods5'],activation='relu'))\n",
    "        model.add(Dropout(calibrers['dropout_rate5']))     \n",
    "        \n",
    "        model.add(Dense(1,activation = calibrers['PrePredictionActivation']))\n",
    "        \n",
    "        model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "    \n",
    "        #print(type(model.summary()))\n",
    "              \n",
    "        # clear_output(wait=True)\n",
    "        print(f'iteration {iteration-1} took {iter_time} s or {iter_time/60} min')\n",
    "    \n",
    "        history = model.fit(training_set_X, training_set_y,validation_data=( test_set_X,  test_set_y), batch_size = int(BATCH_SIZE), epochs = EPOCHS, verbose = 0,callbacks=[],shuffle = True)\n",
    "    \n",
    "    \n",
    "        #training metrics\n",
    "        av_loss_train = np.add(av_loss_train,np.array(history.history['loss']))\n",
    "        av_acc_train = np.add(av_acc_train,np.array(history.history['acc']))\n",
    "        av_prec_train = np.add(av_prec_train,np.array(history.history['precision']))\n",
    "        av_rec_train = np.add(av_rec_train,np.array(history.history['recall']))\n",
    "        av_f1_train = np.add(av_f1_train,np.array(history.history['f1']))\n",
    "        #validation metrics\n",
    "        av_loss_val = np.add(av_loss_val,np.array(history.history['val_loss']))\n",
    "        av_acc_val = np.add(av_acc_val,np.array(history.history['val_acc']))\n",
    "        av_prec_val = np.add(av_prec_val,np.array(history.history['val_precision']))\n",
    "        av_rec_val = np.add(av_rec_val,np.array(history.history['val_recall']))\n",
    "        av_f1_val = np.add(av_f1_val,np.array(history.history['val_f1']))\n",
    "    \n",
    "        end_time = time.time()\n",
    "    \n",
    "        iter_time = end_time-start_time\n",
    "    \n",
    "    av_loss_train = np.divide(av_loss_train,ITERATIONS)\n",
    "    av_acc_train = np.divide(av_acc_train,ITERATIONS)\n",
    "    av_prec_train = np.divide(av_prec_train,ITERATIONS)\n",
    "    av_rec_train = np.divide(av_rec_train,ITERATIONS)\n",
    "    av_f1_train = np.divide(av_f1_train,ITERATIONS)\n",
    "\n",
    "    av_loss_val = np.divide(av_loss_val,ITERATIONS)\n",
    "    av_acc_val = np.divide(av_acc_val,ITERATIONS)\n",
    "    av_prec_val = np.divide(av_prec_val,ITERATIONS)\n",
    "    av_rec_val = np.divide(av_rec_val,ITERATIONS)\n",
    "    av_f1_val = np.divide(av_f1_val,ITERATIONS)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "\n",
    "    results = pd.DataFrame({'train_loss': av_loss_train,'train_acc': av_acc_train,'train_prec': av_prec_train,'train_rec': av_rec_train,'train_f1': av_f1_train,'val_loss': av_loss_val,'val_acc': av_acc_val,'val_prec': av_prec_val,'val_rec': av_rec_val,'val_f1': av_f1_val})\n",
    "    \n",
    "    path = \"runs\"\n",
    "    dirs = os.listdir(path)\n",
    "    dirs.sort(reverse = True)\n",
    "    the_run = dirs[0]\n",
    "    match = re.search(r\"run\\d{5}\", the_run)[0][3:]\n",
    "    match = f'{(int(match)+1):05}'\n",
    "    the_folder = f\"run{match}\"\n",
    "    directory = os.path.join(path, the_folder)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory {the_folder} created!\")\n",
    "    else:\n",
    "        print(\"Directory already exists.\")\n",
    "    \n",
    "    #####################\n",
    "    # now save the results pickle and the hyperparameter text in the correct run folder\n",
    "    filepath = f\"runs\" +f\"/{the_folder}\"\n",
    "    sub_dir1 = os.path.join(filepath,'results.plk')\n",
    "    sub_dir2 = os.path.join(filepath,\"hyperparameters.txt\")\n",
    "    with open(sub_dir1, 'wb') as dummy:\n",
    "        pickle.dump(results, dummy)\n",
    "        \n",
    "    hyperparameters = {\n",
    "                   'ITERATIONS':'5',\n",
    "                   'EPOCHS': '50',\n",
    "                   'LOSS':\"'binary_crossentropy'\",\n",
    "                   'OPTIMIZER': 'Adam(learning_rate=0.0001)',\n",
    "                   'METRICS': \"\"\"['acc', metrics.precision, metrics.recall, metrics.f1]\"\"\",\n",
    "                   'BATCH_SIZE': BATCH_SIZE,\n",
    "                   'TEST_SIZE': '0.3',\n",
    "                   'RELOVIR': RELOVIR }\n",
    "    \n",
    "    \n",
    "    with open(sub_dir2, 'w') as file:\n",
    "        for key in hyperparameters:\n",
    "            file.write('\\n '+ key + ':' + str(hyperparameters[key]) + '\\n')\n",
    "            \n",
    "    # txt with the n maximum val_f1 epochs and their values\n",
    "    n=10\n",
    "    if EPOCHS<n:\n",
    "        n = 1\n",
    "    sub_dir3 = os.path.join(filepath,\"val_f1.txt\")\n",
    "    x = list(results['val_f1'].nlargest(n).index)\n",
    "    print(x)\n",
    "    nums = list(map(lambda x: round(results['val_f1'].loc[x],4),x))\n",
    "    print(nums)\n",
    "    text = 'epoch value\\n'\n",
    "    for i in range(n):\n",
    "        text+=f'-{(x[i]):03}- {nums[i]}\\n'\n",
    "    with open(sub_dir3, 'wb') as dummy:\n",
    "        dummy.write(text.encode())\n",
    "        \n",
    "    #####################\n",
    "    # now save the plot pictures(not particularly usefull in the general code)\n",
    "    plt.plot(results['val_acc'])\n",
    "    plt.plot(results['val_prec'])\n",
    "    plt.plot(results['val_rec'])\n",
    "    plt.plot(results['val_f1'])\n",
    "    plt.title('Average Evaluation Model Metrics')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['accuracy','precision','recall','f1-score'], loc='lower right')#, bbox_to_anchor=(1, 0.5))\n",
    "    plt.savefig(f'runs/{the_folder}/AEMM.jpg',dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "    ######################\n",
    "    # save model summary\n",
    "    with open(f'runs/{the_folder}/model_summary.txt', 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    print(results)\n",
    "    \n",
    "    av_f1_val = np.add(av_f1_val,np.array(history.history['val_f1']))\n",
    " \n",
    "    return -np.max(np.divide(av_f1_val,ITERATIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d3293d5-91c5-419c-ae53-b46f5f12bd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 1)                                             \n",
      "iteration -1 took 0 s or 0.0 min                     \n",
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:Model was constructed with shape (None, 768) for input KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: in user code:\n",
      "\n",
      "    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Exception encountered when calling layer \"sequential_2\" \"                 f\"(type Sequential).\n",
      "    \n",
      "    Input 0 of layer \"dense_12\" is incompatible with the layer: expected axis -1 of input shape to have value 768, but received input with shape (None, 1)\n",
      "    \n",
      "    Call arguments received by layer \"sequential_2\" \"                 f\"(type Sequential):\n",
      "       inputs=tf.Tensor(shape=(None, 1), dtype=string)\n",
      "       training=True\n",
      "       mask=None\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_12\" is incompatible with the layer: expected axis -1 of input shape to have value 768, but received input with shape (None, 1)\n    \n    Call arguments received by layer \"sequential_2\" \"                 f\"(type Sequential):\n       inputs=tf.Tensor(shape=(None, 1), dtype=string)\n       training=True\n       mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[41], line 70\u001b[0m, in \u001b[0;36mmodelize\u001b[1;34m(calibrers)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#print(type(model.summary()))\u001b[39;00m\n\u001b[0;32m     66\u001b[0m       \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# clear_output(wait=True)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_time\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_set_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_set_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#training metrics\u001b[39;00m\n\u001b[0;32m     74\u001b[0m av_loss_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39madd(av_loss_train,np\u001b[38;5;241m.\u001b[39marray(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec_f74dow.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\distef\\AppData\\Local\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_12\" is incompatible with the layer: expected axis -1 of input shape to have value 768, but received input with shape (None, 1)\n    \n    Call arguments received by layer \"sequential_2\" \"                 f\"(type Sequential):\n       inputs=tf.Tensor(shape=(None, 1), dtype=string)\n       training=True\n       mask=None\n"
     ]
    }
   ],
   "source": [
    "best = fmin(fn=modelize, space=hyperspace, algo=tpe.suggest, max_evals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4911480-4f5b-4d3b-885e-a02901ba31f8",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b64f54b-cb33-4e3e-8b4c-510c03e75e94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/\n",
      "    run00000/\n",
      "    run00001/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00002/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00003/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00004/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00005/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00006/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00007/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00008/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00009/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00010/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00011/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00012/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00013/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00014/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00015/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00016/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00017/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00018/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00019/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00020/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00021/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00022/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00023/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00024/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00025/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00026/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00027/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00028/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00029/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00030/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00031/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00032/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00033/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00034/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00035/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00036/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00037/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00038/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00039/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00040/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00041/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00042/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00043/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00044/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00045/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00046/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00047/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00048/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00049/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n",
      "    run00050/\n",
      "        AEMM.jpg\n",
      "        hyperparameters.txt\n",
      "        model_summary.txt\n",
      "        results.plk\n",
      "        val_f1.txt\n"
     ]
    }
   ],
   "source": [
    "def print_directory_structure(root_directory):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "        level = dirpath.replace(root_directory, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(dirpath)))\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in filenames:\n",
    "            print('{}{}'.format(sub_indent, f))\n",
    "root_directory = 'runs'\n",
    "print_directory_structure(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d6a78f8d-e38c-4e04-a30c-656c053d43e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxlist = []\n",
    "runlist = []\n",
    "for run in os.listdir('runs')[1:]:\n",
    "    folderpath = 'runs/'+run\n",
    "    hyperparameters = None\n",
    "    results = None\n",
    "    with open(folderpath+'/results.plk', 'rb') as file:\n",
    "        results = pickle.load(file)\n",
    "    results = results.sort_values(by='val_f1',ascending = False)\n",
    "    maxlist.append(results['val_f1'][0])\n",
    "    runlist.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2b0fa662-de98-41fa-9934-e9cafda7a18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({'run':runlist,'f1':maxlist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f8bcc3b-0def-4a7e-ae9b-ed2f416f6071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison = comparison.sort_values(by = 'f1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75f0618d-bcf5-41ab-ad3f-1bd292b17508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>run00035</td>\n",
       "      <td>0.951997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>run00024</td>\n",
       "      <td>0.951599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>run00022</td>\n",
       "      <td>0.948773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>run00045</td>\n",
       "      <td>0.946928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>run00016</td>\n",
       "      <td>0.946694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>run00021</td>\n",
       "      <td>0.945542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>run00023</td>\n",
       "      <td>0.941215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>run00008</td>\n",
       "      <td>0.940777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>run00032</td>\n",
       "      <td>0.936858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run00002</td>\n",
       "      <td>0.935368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>run00029</td>\n",
       "      <td>0.928909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>run00033</td>\n",
       "      <td>0.926815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>run00025</td>\n",
       "      <td>0.922047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>run00044</td>\n",
       "      <td>0.921236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run00005</td>\n",
       "      <td>0.920211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>run00036</td>\n",
       "      <td>0.918539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run00001</td>\n",
       "      <td>0.914049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>run00020</td>\n",
       "      <td>0.913186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>run00030</td>\n",
       "      <td>0.907543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>run00050</td>\n",
       "      <td>0.904019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>run00049</td>\n",
       "      <td>0.900850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>run00019</td>\n",
       "      <td>0.900148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>run00009</td>\n",
       "      <td>0.896059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>run00042</td>\n",
       "      <td>0.892042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>run00040</td>\n",
       "      <td>0.890738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>run00010</td>\n",
       "      <td>0.888859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>run00012</td>\n",
       "      <td>0.886379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>run00043</td>\n",
       "      <td>0.882632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>run00048</td>\n",
       "      <td>0.878018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>run00038</td>\n",
       "      <td>0.877357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>run00007</td>\n",
       "      <td>0.875267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run00004</td>\n",
       "      <td>0.875236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>run00037</td>\n",
       "      <td>0.870488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>run00015</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>run00017</td>\n",
       "      <td>0.865882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>run00011</td>\n",
       "      <td>0.857190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>run00027</td>\n",
       "      <td>0.855291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>run00047</td>\n",
       "      <td>0.852219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>run00039</td>\n",
       "      <td>0.848875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>run00026</td>\n",
       "      <td>0.847368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>run00006</td>\n",
       "      <td>0.843165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>run00041</td>\n",
       "      <td>0.839719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>run00046</td>\n",
       "      <td>0.836620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>run00018</td>\n",
       "      <td>0.832054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>run00034</td>\n",
       "      <td>0.811778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>run00013</td>\n",
       "      <td>0.810668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>run00028</td>\n",
       "      <td>0.810239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run00003</td>\n",
       "      <td>0.809776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>run00031</td>\n",
       "      <td>0.808566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>run00014</td>\n",
       "      <td>0.805890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         run        f1\n",
       "34  run00035  0.951997\n",
       "23  run00024  0.951599\n",
       "21  run00022  0.948773\n",
       "44  run00045  0.946928\n",
       "15  run00016  0.946694\n",
       "20  run00021  0.945542\n",
       "22  run00023  0.941215\n",
       "7   run00008  0.940777\n",
       "31  run00032  0.936858\n",
       "1   run00002  0.935368\n",
       "28  run00029  0.928909\n",
       "32  run00033  0.926815\n",
       "24  run00025  0.922047\n",
       "43  run00044  0.921236\n",
       "4   run00005  0.920211\n",
       "35  run00036  0.918539\n",
       "0   run00001  0.914049\n",
       "19  run00020  0.913186\n",
       "29  run00030  0.907543\n",
       "49  run00050  0.904019\n",
       "48  run00049  0.900850\n",
       "18  run00019  0.900148\n",
       "8   run00009  0.896059\n",
       "41  run00042  0.892042\n",
       "39  run00040  0.890738\n",
       "9   run00010  0.888859\n",
       "11  run00012  0.886379\n",
       "42  run00043  0.882632\n",
       "47  run00048  0.878018\n",
       "37  run00038  0.877357\n",
       "6   run00007  0.875267\n",
       "3   run00004  0.875236\n",
       "36  run00037  0.870488\n",
       "14  run00015  0.869323\n",
       "16  run00017  0.865882\n",
       "10  run00011  0.857190\n",
       "26  run00027  0.855291\n",
       "46  run00047  0.852219\n",
       "38  run00039  0.848875\n",
       "25  run00026  0.847368\n",
       "5   run00006  0.843165\n",
       "40  run00041  0.839719\n",
       "45  run00046  0.836620\n",
       "17  run00018  0.832054\n",
       "33  run00034  0.811778\n",
       "12  run00013  0.810668\n",
       "27  run00028  0.810239\n",
       "2   run00003  0.809776\n",
       "30  run00031  0.808566\n",
       "13  run00014  0.805890"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4139216-e1e1-4693-8b30-e997bec6bc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
